{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../../analysis/data/nbmodel_templates.csv'\n",
    "BATCH_SIZE = 100\n",
    "HIDDEN_DIM = 100\n",
    "SEQ_LENGTH = 50\n",
    "WEIGHTS = ''\n",
    "\n",
    "GENERATE_LENGTH = 100\n",
    "LAYER_NUM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are report, report_chunks, template_chunks, templates\n",
      "Processed 1953 lines.\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "with open(DATA_DIR, encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            reports.append(row[3].replace('\"', '').replace(\"[\", '').replace(']', ''))\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313725"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = ' '.join(reports).split(' ')\n",
    "data = np.array(reports).flatten()\n",
    "data = ' '.join(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'${player_n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words:\n",
      "['7', 'u', 'l', 'V', 'C', 'g', 'b', 'p', 'G', '3', 'e', 'i', 'T', 'D', 'R', ',', 'r', 'J', '0', '8', 'v', 'h', '1', 'y', 'X', '(', 'x', '_', 'd', '-', 'H', '5', '4', 't', 'B', 'W', 'w', 'I', 'E', '6', 'M', 'N', 'k', 'o', 'L', 'A', 'F', 's', '$', 'n', 'K', 'S', 'q', '}', '.', 'P', 'a', 'f', 'U', '{', '2', 'j', ')', 'z', 'm', '9', 'O', 'c', \"'\", ' ']\n",
      "\n",
      "VOCAB_SIZE: 70\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data)) #set: gets unique values\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "print('Unique Words:\\n{}\\n\\nVOCAB_SIZE: {}'.format(chars, VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = {i: char for i, char in enumerate(chars)}\n",
    "char_to_idx = {char: i for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEQ_LENGTH = 60 #input sequence length\n",
    "N_FEATURES = VOCAB_SIZE #one hot encoding here, that's why, but deduplicated for clarity\n",
    "\n",
    "N_SEQ = int(np.floor((len(data) - 1) / SEQ_LENGTH))\n",
    "\n",
    "X = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
    "y = np.zeros((N_SEQ, SEQ_LENGTH, N_FEATURES))\n",
    "\n",
    "for i in range(N_SEQ):\n",
    "  X_sequence = data[i * SEQ_LENGTH: (i + 1) * SEQ_LENGTH]\n",
    "  X_sequence_ix = [char_to_idx[c] for c in X_sequence]\n",
    "  input_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "  for j in range(SEQ_LENGTH):\n",
    "    input_sequence[j][X_sequence_ix[j]] = 1. #one-hot encoding of the input characters\n",
    "  X[i] = input_sequence\n",
    "  \n",
    "  y_sequence = data[i * SEQ_LENGTH + 1: (i + 1) * SEQ_LENGTH + 1] #shifted by 1 to the right\n",
    "  y_sequence_ix = [char_to_idx[c] for c in y_sequence]\n",
    "  target_sequence = np.zeros((SEQ_LENGTH, N_FEATURES))\n",
    "  for j in range(SEQ_LENGTH):\n",
    "    target_sequence[j][y_sequence_ix[j]] = 1. #one-hot encoding of the target characters\n",
    "  y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, TimeDistributed, Dense, Activation\n",
    "\n",
    "# constant parameter for the model\n",
    "HIDDEN_DIM = 700 #size of each hidden layer, \"each layer has 700 hidden states\"\n",
    "LAYER_NUM = 2 #number of hidden layers, how much were used?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, \n",
    "               input_shape=(None, VOCAB_SIZE), \n",
    "               return_sequences=True))\n",
    "for _ in range(LAYER_NUM - 1):\n",
    "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length):\n",
    "  ix = [np.random.randint(VOCAB_SIZE)]\n",
    "  y_char = [idx_to_char[ix[-1]]]\n",
    "  X = np.zeros((1, length, VOCAB_SIZE))\n",
    "  for i in range(length):\n",
    "    X[0, i, :][ix[-1]] = 1.\n",
    "    ix = np.argmax(model.predict(X[:, :i+1,:])[0], 1)\n",
    "    print(idx_to_char[ix[-1]], end=\"\")\n",
    "    y_char.append(idx_to_char[ix[-1]])\n",
    "#   return ''.join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "# callback to save the model if better\n",
    "filepath=\"tgt_model.hdf5\"\n",
    "save_model_cb = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# callback to stop the training if no improvement\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=4)\n",
    "# callback to generate text at epoch end\n",
    "class generateText(Callback):\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        pass\n",
    "#         print(generate_text(self.model, 100))\n",
    "        \n",
    "generate_text_cb = generateText()\n",
    "\n",
    "callbacks_list = [save_model_cb, early_stopping_cb, generate_text_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4705 samples, validate on 523 samples\n",
      "Epoch 1/200\n",
      "4705/4705 [==============================] - 27s 6ms/step - loss: 3.2795 - acc: 0.1306 - val_loss: 2.9392 - val_acc: 0.1862\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.18620, saving model to tgt_model.hdf5\n",
      "Epoch 2/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 2.6587 - acc: 0.2361 - val_loss: 2.2135 - val_acc: 0.3458\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.18620 to 0.34579, saving model to tgt_model.hdf5\n",
      "Epoch 3/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 1.7611 - acc: 0.4794 - val_loss: 1.2169 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.34579 to 0.68489, saving model to tgt_model.hdf5\n",
      "Epoch 4/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 1.0912 - acc: 0.6938 - val_loss: 1.1484 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.68489\n",
      "Epoch 5/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.7045 - acc: 0.8089 - val_loss: 0.5174 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.68489 to 0.86536, saving model to tgt_model.hdf5\n",
      "Epoch 6/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.5187 - acc: 0.8588 - val_loss: 0.3834 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.86536 to 0.90118, saving model to tgt_model.hdf5\n",
      "Epoch 7/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.3807 - acc: 0.8970 - val_loss: 0.3352 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.90118 to 0.91007, saving model to tgt_model.hdf5\n",
      "Epoch 8/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.3288 - acc: 0.9091 - val_loss: 0.2700 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.91007 to 0.92451, saving model to tgt_model.hdf5\n",
      "Epoch 9/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.2759 - acc: 0.9215 - val_loss: 0.2638 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92451 to 0.92578, saving model to tgt_model.hdf5\n",
      "Epoch 10/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.2427 - acc: 0.9283 - val_loss: 0.2296 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.92578 to 0.93397, saving model to tgt_model.hdf5\n",
      "Epoch 11/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.2204 - acc: 0.9338 - val_loss: 0.2141 - val_acc: 0.9382\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.93397 to 0.93821, saving model to tgt_model.hdf5\n",
      "Epoch 12/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.2019 - acc: 0.9381 - val_loss: 0.1939 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.93821 to 0.94302, saving model to tgt_model.hdf5\n",
      "Epoch 13/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1862 - acc: 0.9414 - val_loss: 0.1896 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.94302 to 0.94414, saving model to tgt_model.hdf5\n",
      "Epoch 14/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1742 - acc: 0.9440 - val_loss: 0.1799 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.94414 to 0.94490, saving model to tgt_model.hdf5\n",
      "Epoch 15/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1639 - acc: 0.9466 - val_loss: 0.1750 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.94490 to 0.94493, saving model to tgt_model.hdf5\n",
      "Epoch 16/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1556 - acc: 0.9484 - val_loss: 0.1727 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.94493 to 0.94716, saving model to tgt_model.hdf5\n",
      "Epoch 17/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1497 - acc: 0.9498 - val_loss: 0.1728 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.94716\n",
      "Epoch 18/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1434 - acc: 0.9513 - val_loss: 0.1730 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.94716\n",
      "Epoch 19/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1389 - acc: 0.9525 - val_loss: 0.1678 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.94716 to 0.94793, saving model to tgt_model.hdf5\n",
      "Epoch 20/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1347 - acc: 0.9536 - val_loss: 0.1666 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.94793\n",
      "Epoch 21/200\n",
      "4705/4705 [==============================] - 23s 5ms/step - loss: 0.1317 - acc: 0.9545 - val_loss: 0.1633 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.94793 to 0.94812, saving model to tgt_model.hdf5\n",
      "Epoch 22/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1285 - acc: 0.9552 - val_loss: 0.1623 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.94812 to 0.94815, saving model to tgt_model.hdf5\n",
      "Epoch 23/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1258 - acc: 0.9559 - val_loss: 0.1668 - val_acc: 0.9478\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.94815\n",
      "Epoch 24/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1245 - acc: 0.9559 - val_loss: 0.1636 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.94815 to 0.94841, saving model to tgt_model.hdf5\n",
      "Epoch 25/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1227 - acc: 0.9566 - val_loss: 0.1629 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.94841 to 0.94968, saving model to tgt_model.hdf5\n",
      "Epoch 26/200\n",
      "4705/4705 [==============================] - 24s 5ms/step - loss: 0.1209 - acc: 0.9566 - val_loss: 0.1626 - val_acc: 0.9492\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.94968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf032a9b38>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_EPOCHS = 200 #max number of epochs to train, \"200 epochs\"\n",
    "BATCH_SIZE = 128 \n",
    "VALIDATION_SPLIT = 0.1 #proportion of the batch used for validation at each epoch\n",
    "\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=NB_EPOCHS, \n",
    "          callbacks=callbacks_list, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er the ${opp}. ${player_name} caught ${receptions} of ${rec_targets} targets for ${rec_yards} yards in the ${team}' week ${week} win over the ${opp}. ${player_name} caught ${receptions} of ${rec_targe"
     ]
    }
   ],
   "source": [
    "generate_text(model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
