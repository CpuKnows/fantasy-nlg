{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('../../data/player_list.csv')\n",
    "weekly_stats = pd.read_csv('../../data/player_stats.csv')\n",
    "news = pd.read_csv('../../data/player_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all player stats and the corresponding news article in one row\n",
    "full_df = players.drop(columns='player_link')\n",
    "temp_df = news.rename(index=str, columns={'date': 'news_date'})\n",
    "temp_df['news_date'] = pd.to_datetime(temp_df['news_date']).dt.date\n",
    "temp_df = temp_df.drop(columns=['player', 'position', 'url'])\n",
    "full_df = pd.merge(full_df, temp_df, on='player_id')\n",
    "temp_df = weekly_stats.rename(index=str, columns={'date': 'stats_date'})\n",
    "temp_df['stats_date'] = pd.to_datetime(temp_df['stats_date']).dt.date\n",
    "temp_df = temp_df.drop(columns='player')\n",
    "full_df = pd.merge(full_df, temp_df, left_on=['player_id', 'news_date'], right_on=['player_id', 'stats_date'])\n",
    "full_df = full_df.drop(columns='stats_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('../../data/news_and_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../../data/news_and_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "from string import Template\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_word_dict = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', \n",
    "                    'ten': '10', 'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fouteen': '14', 'fifteen': '15', 'sixteen': '16', \n",
    "                    'seventeen': '17', 'eighteen': '18', 'nineteen': '19', 'twenty': '20', 'thirty': '30', 'fourty': '40', 'fifty': '50', \n",
    "                    'sixty': '60', 'seventy': '70', 'eighty': '80', 'ninety': '90'}\n",
    "\n",
    "def number_words_repl(match):\n",
    "    return number_word_dict[match.group(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['player_name', 'player_position', 'team', 'week', 'opp', 'away_game', \n",
    "             'pass_attempts', 'pass_completions', 'pass_percent', 'pass_yards', 'pass_ya', 'pass_td', 'pass_int', \n",
    "             'rush_attempts', 'rush_yards', 'rush_avg', 'rush_td', \n",
    "             'reception', 'rec_yards', 'rec_avg', 'rec_td', \n",
    "             'fumb_lost', 'ko_ret_td', 'ko_ret_yards', 'punt_ret_td', 'punt_ret_yards']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "\n",
    "class NFLTeamRecognizer(object):\n",
    "    name = 'nfl_teams'\n",
    "    \n",
    "    def __init__(self, nlp, teams=tuple(), label='NFL_TEAM'):\n",
    "        if label in nlp.vocab.strings:\n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        else:\n",
    "            nlp.vocab.strings.add(label) \n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        patterns = [nlp(team) for team in teams]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('NFL_TEAMS', None, *patterns)\n",
    "        \n",
    "        Token.set_extension('is_nfl_team', default=False)\n",
    "        Doc.set_extension('has_nfl_team', getter=self.has_nfl_team)\n",
    "        Span.set_extension('has_nfl_team', getter=self.has_nfl_team)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for _, start, end in matches:\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            for token in entity:\n",
    "                token._.set('is_nfl_team', True)\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "        return doc\n",
    "    \n",
    "    def has_nfl_team(self, tokens):\n",
    "        return any([t._.get('is_nfl_team') for t in tokens])\n",
    "    \n",
    "\n",
    "class NFLPlayerRecognizer(object):\n",
    "    name = 'nfl_players'\n",
    "    \n",
    "    def __init__(self, nlp, players=tuple(), label='NFL_PLAYER'):\n",
    "        if label in nlp.vocab.strings:\n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        else:\n",
    "            nlp.vocab.strings.add(label) \n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        patterns = [nlp(player) for player in players]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('NFL_PLAYERS', None, *patterns)\n",
    "        \n",
    "        Token.set_extension('is_nfl_player', default=False)\n",
    "        Doc.set_extension('has_nfl_player', getter=self.has_nfl_player)\n",
    "        Span.set_extension('has_nfl_player', getter=self.has_nfl_player)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for _, start, end in matches:\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            for token in entity:\n",
    "                token._.set('is_nfl_player', True)\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "        return doc\n",
    "    \n",
    "    def has_nfl_player(self, tokens):\n",
    "        return any([t._.get('is_nfl_player') for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams\n",
    "with open('../../data/teams_aliases.txt') as f:\n",
    "    team_dict = eval(f.read())\n",
    "    teams = [v for v in team_dict.values()]\n",
    "    teams = list(chain(*teams))\n",
    "    \n",
    "id_team_dict = dict()\n",
    "id_val = 0\n",
    "for k, v in team_dict.items():\n",
    "    id_team_dict[id_val] = v\n",
    "    id_val += 1\n",
    "    \n",
    "team_id_dict = dict()\n",
    "for k, v in id_team_dict.items():\n",
    "    for v_i in v:\n",
    "        team_id_dict[v_i] = k\n",
    "\n",
    "# Players\n",
    "players = pd.read_csv('../../data/player_list.csv')\n",
    "player_list = players['player_name'].tolist()\n",
    "player_list.extend([i[-1] for i in players['player_name'].str.split(' ')])\n",
    "player_list = set(player_list)\n",
    "    \n",
    "component = NFLTeamRecognizer(nlp, teams)\n",
    "nlp.add_pipe(component, last=True)\n",
    "component = NFLPlayerRecognizer(nlp, player_list)\n",
    "nlp.add_pipe(component, last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict):\n",
    "    inverted_news_dict = dict()\n",
    "    \n",
    "    for k, v in news_dict.items():\n",
    "        if k in data_cols and v is not np.nan:\n",
    "            if (type(v) is np.float64 or type(v) is float) and v % 1 == 0:\n",
    "                v = int(v)\n",
    "            k, v = str(k), str(v)\n",
    "\n",
    "            if k in ['team', 'opp']:\n",
    "                team_id = team_id_dict[v]\n",
    "                team_surface_forms = id_team_dict[team_id]\n",
    "                for team in team_surface_forms:\n",
    "                    inverted_news_dict[team] = k\n",
    "            elif v not in inverted_news_dict:\n",
    "                inverted_news_dict[v] = k\n",
    "            elif type(inverted_news_dict[v]) is list:\n",
    "                inverted_news_dict[v] = inverted_news_dict[v] + [k]\n",
    "            else:\n",
    "                inverted_news_dict[v] = [inverted_news_dict[v], k]\n",
    "                \n",
    "    return inverted_news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text, number_word_dict):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.replace('-of-', ' of ')\n",
    "    text = re.sub(r'\\bWeek\\b', 'week', text)\n",
    "    text = re.sub(r'\\b(' + '|'.join([k for k in number_word_dict.keys()]) + r')\\b', \n",
    "                  number_words_repl, text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(\\d+)-([A-Za-z]+)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'(\\[A-Za-z]+)-(\\d+)', r'\\1 \\2', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_template(doc, inverted_news_dict):\n",
    "    tagged_text = ''\n",
    "    ambiguous_placeholder_count = 0\n",
    "    ambiguous_placeholder_dict = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in inverted_news_dict:\n",
    "            if type(inverted_news_dict[token.text]) is list:\n",
    "                tagged_text += '${temp_var_' + str(ambiguous_placeholder_count) + '}'\n",
    "                ambiguous_placeholder_dict['temp_var_{}'.format(ambiguous_placeholder_count)] = inverted_news_dict[token.text]\n",
    "                ambiguous_placeholder_count += 1\n",
    "            else:\n",
    "                tagged_text += '${' + inverted_news_dict[token.text] + '}'\n",
    "        else:\n",
    "            tagged_text += token.text.replace('$', '$$')\n",
    "        tagged_text += token.text_with_ws[len(token.text):]\n",
    "        \n",
    "    return Template(tagged_text), ambiguous_placeholder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_reports = []\n",
    "output_templates = []\n",
    "ambiguous_placeholder_list = []\n",
    "\n",
    "for row in full_df.iterrows():\n",
    "    news_dict = row[1].to_dict()\n",
    "    inverted_news_dict = create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict)\n",
    "    \n",
    "    normalized_text = text_normalization(news_dict['report'], number_word_dict)\n",
    "                \n",
    "    doc = nlp(normalized_text)\n",
    "    for e in doc.ents:\n",
    "        if e.label_ in ['NFL_PLAYER', 'NFL_TEAM']:\n",
    "            e.merge()\n",
    "    \n",
    "    news_template, ambiguous_placeholders = doc_to_template(doc, inverted_news_dict)\n",
    "    news_reports.append(news_dict['report'])\n",
    "    output_templates.append(news_template.template)\n",
    "    ambiguous_placeholder_list.append(ambiguous_placeholders)\n",
    "    \n",
    "pd.DataFrame({'reports': news_reports, 'templates': output_templates}).to_csv('../../data/output_templates.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fantasy_nlg]",
   "language": "python",
   "name": "conda-env-fantasy_nlg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
