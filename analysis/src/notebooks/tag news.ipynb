{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('../../data/player_list.csv')\n",
    "weekly_stats = pd.read_csv('../../data/player_stats.csv')\n",
    "news = pd.read_csv('../../data/player_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbdb_stats = pd.read_csv('../../data/football_db_player_stats.csv')\n",
    "fbdb_stats['date'] = pd.to_datetime(fbdb_stats['date'])\n",
    "fbdb_stats = fbdb_stats.loc[lambda df: df['date'] >= '2018-09-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_week = pd.DataFrame({'week_period': pd.period_range('2018-09-06', periods=17, freq='W-WED'),\n",
    "                         'week': np.arange(1, 18)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all player stats and the corresponding news article in one row\n",
    "full_df = fbdb_stats.drop(columns=['url', 'team', 'pass_lg', 'rush_lg', 'rec_lg', 'rush_fd', 'rec_fd'])\n",
    "full_df['week_period'] = full_df['date'].dt.to_period('W-WED')\n",
    "full_df['date'] = full_df['date'].dt.date\n",
    "full_df['opp'] = full_df['opp'].str.upper()\n",
    "full_df['opp'] = full_df['opp'].replace('LA', 'LAR')\n",
    "full_df = pd.merge(full_df, nfl_week, how='left', on='week_period')\n",
    "full_df['week'] = full_df['week'].astype(int)\n",
    "full_df = full_df.drop(columns=['week_period'])\n",
    "\n",
    "temp_df = news.drop(columns=['player_id'])\n",
    "temp_df = temp_df.rename(index=str, columns={'player': 'player_name', 'position': 'player_position'})\n",
    "temp_df['date'] = pd.to_datetime(temp_df['date']).dt.date\n",
    "full_df = pd.merge(full_df, temp_df, on=['player_name', 'player_position', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all player stats and the corresponding news article in one row\n",
    "full_df = players.drop(columns='player_link')\n",
    "temp_df = news.rename(index=str, columns={'date': 'news_date'})\n",
    "temp_df['news_date'] = pd.to_datetime(temp_df['news_date']).dt.date\n",
    "temp_df = temp_df.drop(columns=['player', 'position', 'url'])\n",
    "full_df = pd.merge(full_df, temp_df, on='player_id')\n",
    "temp_df = weekly_stats.rename(index=str, columns={'date': 'stats_date'})\n",
    "temp_df['stats_date'] = pd.to_datetime(temp_df['stats_date']).dt.date\n",
    "temp_df = temp_df.drop(columns='player')\n",
    "full_df = pd.merge(full_df, temp_df, left_on=['player_id', 'news_date'], right_on=['player_id', 'stats_date'])\n",
    "full_df = full_df.drop(columns='stats_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('../../data/news_and_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('../../data/news_and_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "from string import Template\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_word_dict = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', \n",
    "                    'ten': '10', 'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fouteen': '14', 'fifteen': '15', 'sixteen': '16', \n",
    "                    'seventeen': '17', 'eighteen': '18', 'nineteen': '19', 'twenty': '20', 'thirty': '30', 'fourty': '40', 'fifty': '50', \n",
    "                    'sixty': '60', 'seventy': '70', 'eighty': '80', 'ninety': '90'}\n",
    "\n",
    "def number_words_repl(match):\n",
    "    return number_word_dict[match.group(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['player_name', 'player_position', 'team', 'week', 'opp', 'away_game', 'team_score', 'opp_score',\n",
    "             'pass_attempts', 'pass_completions', 'pass_percent', 'pass_yards', 'pass_ya', 'pass_td', 'pass_int', 'pass_sack', 'pass_rate', \n",
    "             'rush_attempts', 'rush_yards', 'rush_avg', 'rush_td', \n",
    "             'receptions', 'rec_yards', 'rec_avg', 'rec_td', 'rec_targets', 'rec_yac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['player_name', 'player_position', 'team', 'week', 'opp', 'away_game', \n",
    "             'pass_attempts', 'pass_completions', 'pass_percent', 'pass_yards', 'pass_ya', 'pass_td', 'pass_int', \n",
    "             'rush_attempts', 'rush_yards', 'rush_avg', 'rush_td', \n",
    "             'reception', 'rec_yards', 'rec_avg', 'rec_td', \n",
    "             'fumb_lost', 'ko_ret_td', 'ko_ret_yards', 'punt_ret_td', 'punt_ret_yards']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "\n",
    "class NFLTeamRecognizer(object):\n",
    "    name = 'nfl_teams'\n",
    "    \n",
    "    def __init__(self, nlp, teams=tuple(), label='NFL_TEAM'):\n",
    "        if label in nlp.vocab.strings:\n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        else:\n",
    "            nlp.vocab.strings.add(label) \n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        patterns = [nlp(team) for team in teams]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('NFL_TEAMS', None, *patterns)\n",
    "        \n",
    "        Token.set_extension('is_nfl_team', default=False)\n",
    "        Doc.set_extension('has_nfl_team', getter=self.has_nfl_team)\n",
    "        Span.set_extension('has_nfl_team', getter=self.has_nfl_team)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for _, start, end in matches:\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            for token in entity:\n",
    "                token._.set('is_nfl_team', True)\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "        return doc\n",
    "    \n",
    "    def has_nfl_team(self, tokens):\n",
    "        return any([t._.get('is_nfl_team') for t in tokens])\n",
    "    \n",
    "\n",
    "class NFLPlayerRecognizer(object):\n",
    "    name = 'nfl_players'\n",
    "    \n",
    "    def __init__(self, nlp, players=tuple(), label='NFL_PLAYER'):\n",
    "        if label in nlp.vocab.strings:\n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        else:\n",
    "            nlp.vocab.strings.add(label) \n",
    "            self.label = nlp.vocab.strings[label]\n",
    "        patterns = [nlp(player) for player in players]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('NFL_PLAYERS', None, *patterns)\n",
    "        \n",
    "        Token.set_extension('is_nfl_player', default=False)\n",
    "        Doc.set_extension('has_nfl_player', getter=self.has_nfl_player)\n",
    "        Span.set_extension('has_nfl_player', getter=self.has_nfl_player)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for _, start, end in matches:\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            for token in entity:\n",
    "                token._.set('is_nfl_player', True)\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "        return doc\n",
    "    \n",
    "    def has_nfl_player(self, tokens):\n",
    "        return any([t._.get('is_nfl_player') for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams\n",
    "with open('../../data/teams_aliases.txt') as f:\n",
    "    team_dict = eval(f.read())\n",
    "    teams = [v for v in team_dict.values()]\n",
    "    teams = list(chain(*teams))\n",
    "    \n",
    "id_team_dict = dict()\n",
    "id_val = 0\n",
    "for k, v in team_dict.items():\n",
    "    id_team_dict[id_val] = v\n",
    "    id_val += 1\n",
    "    \n",
    "team_id_dict = dict()\n",
    "for k, v in id_team_dict.items():\n",
    "    for v_i in v:\n",
    "        team_id_dict[v_i] = k\n",
    "\n",
    "# Players\n",
    "players = pd.read_csv('../../data/player_list.csv')\n",
    "player_list = players['player_name'].tolist()\n",
    "player_list.extend([i[-1] for i in players['player_name'].str.split(' ')])\n",
    "player_list = set(player_list)\n",
    "    \n",
    "component = NFLTeamRecognizer(nlp, teams)\n",
    "nlp.add_pipe(component, last=True)\n",
    "component = NFLPlayerRecognizer(nlp, player_list)\n",
    "nlp.add_pipe(component, last=True)\n",
    "Token.set_extension('template_tag', default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict):\n",
    "    inverted_news_dict = dict()\n",
    "    \n",
    "    for k, v in news_dict.items():\n",
    "        if k in data_cols and v is not np.nan:\n",
    "            if (type(v) is np.float64 or type(v) is float) and v % 1 == 0:\n",
    "                v = int(v)\n",
    "            k, v = str(k), str(v)\n",
    "\n",
    "            if k in ['team', 'opp']:\n",
    "                team_id = team_id_dict[v]\n",
    "                team_surface_forms = id_team_dict[team_id]\n",
    "                for team in team_surface_forms:\n",
    "                    inverted_news_dict[team] = k\n",
    "            elif v not in inverted_news_dict:\n",
    "                inverted_news_dict[v] = k\n",
    "            elif type(inverted_news_dict[v]) is list:\n",
    "                inverted_news_dict[v] = inverted_news_dict[v] + [k]\n",
    "            else:\n",
    "                inverted_news_dict[v] = [inverted_news_dict[v], k]\n",
    "                \n",
    "    return inverted_news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text, number_word_dict):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = text.replace('-of-', ' of ')\n",
    "    text = re.sub(r'\\bWeek\\b', 'week', text)\n",
    "    text = re.sub(r'\\b(' + '|'.join([k for k in number_word_dict.keys()]) + r')\\b', \n",
    "                  number_words_repl, text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(\\d+)-([A-Za-z]+)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'(\\[A-Za-z]+)-(\\d+)', r'\\1 \\2', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_template(doc, inverted_news_dict, return_doc=False):\n",
    "    tagged_text = ''\n",
    "    ambiguous_placeholder_count = 0\n",
    "    ambiguous_placeholder_dict = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in inverted_news_dict:\n",
    "            if type(inverted_news_dict[token.text]) is list:\n",
    "                tagged_text += '${temp_var_' + str(ambiguous_placeholder_count) + '}'\n",
    "                ambiguous_placeholder_dict['temp_var_{}'.format(ambiguous_placeholder_count)] = inverted_news_dict[token.text]\n",
    "                ambiguous_placeholder_count += 1\n",
    "                token._.template_tag = 'temp_var_' + str(ambiguous_placeholder_count)\n",
    "            else:\n",
    "                tagged_text += '${' + inverted_news_dict[token.text] + '}'\n",
    "                token._.template_tag = inverted_news_dict[token.text]\n",
    "        else:\n",
    "            tagged_text += token.text.replace('$', '$$')\n",
    "        tagged_text += token.text_with_ws[len(token.text):]\n",
    "    \n",
    "    if return_doc:\n",
    "        return Template(tagged_text), ambiguous_placeholder_dict, doc\n",
    "    else:\n",
    "        return Template(tagged_text), ambiguous_placeholder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_reports = []\n",
    "output_templates = []\n",
    "ambiguous_placeholder_list = []\n",
    "\n",
    "for row in full_df.iterrows():\n",
    "    news_dict = row[1].to_dict()\n",
    "    inverted_news_dict = create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict)\n",
    "    \n",
    "    normalized_text = text_normalization(news_dict['report'], number_word_dict)\n",
    "                \n",
    "    doc = nlp(normalized_text)\n",
    "    for e in doc.ents:\n",
    "        if e.label_ in ['NFL_PLAYER', 'NFL_TEAM']:\n",
    "            e.merge()\n",
    "    \n",
    "    news_template, ambiguous_placeholders = doc_to_template(doc, inverted_news_dict)\n",
    "    news_reports.append(news_dict['report'])\n",
    "    output_templates.append(news_template.template)\n",
    "    ambiguous_placeholder_list.append(ambiguous_placeholders)\n",
    "    \n",
    "pd.DataFrame({'reports': news_reports, 'templates': output_templates}).to_csv('../../data/output_templates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = full_df.loc[22].to_dict()\n",
    "inverted_news_dict = create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[lambda df: df['player_name'] == 'Dak Prescott'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[0, 'report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = full_df.loc[0].to_dict()\n",
    "inverted_news_dict = create_inverted_news_dict(news_dict, data_cols, team_id_dict, id_team_dict)\n",
    "\n",
    "normalized_text = text_normalization(news_dict['report'], number_word_dict)\n",
    "\n",
    "doc = nlp(normalized_text)\n",
    "for e in doc.ents:\n",
    "    if e.label_ in ['NFL_PLAYER', 'NFL_TEAM']:\n",
    "        e.merge()\n",
    "\n",
    "news_template, ambiguous_placeholders, doc = doc_to_template(doc, inverted_news_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token._.template_tag, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_tag_context_ngrams(doc):\n",
    "    unigram_l = None\n",
    "    unigram_r = None\n",
    "    bigram_l = None\n",
    "    bigram_r = None\n",
    "    context_ngrams = {}\n",
    "    context = []\n",
    "    bad_pos = ['NUM', 'PART', 'PUNCT', 'SYM', 'X', 'SPACE']\n",
    "\n",
    "    for token in doc:\n",
    "        if token._.template_tag is not None:\n",
    "            if token.i > 0:\n",
    "                if token.nbor(-1)._.template_tag is not None:\n",
    "                    unigram_l = token.nbor(-1)._.template_tag\n",
    "                else:\n",
    "                    unigram_l = token.nbor(-1).text\n",
    "                context.append(unigram_l)\n",
    "            if token.i > 1:\n",
    "                if token.nbor(2)._.template_tag is not None:\n",
    "                    bigram_l = token.nbor(-2)._.template_tag\n",
    "                else:\n",
    "                    bigram_l = token.nbor(-2).text\n",
    "                context.append(bigram_l + ' ' + unigram_l)\n",
    "            if token.i < len(doc) - 1:\n",
    "                if token.nbor(1)._.template_tag is not None:\n",
    "                    unigram_r = token.nbor(1)._.template_tag\n",
    "                else:\n",
    "                    unigram_r = token.nbor(1).text\n",
    "                context.append(unigram_r)\n",
    "            if token.i < len(doc) - 2:\n",
    "                if token.nbor(2)._.template_tag is not None:\n",
    "                    bigram_r = token.nbor(2)._.template_tag\n",
    "                else:\n",
    "                    bigram_r = token.nbor(2).text\n",
    "                context.append(unigram_r + ' ' + bigram_r)\n",
    "                \n",
    "            context_ngrams[token._.template_tag] = context\n",
    "        \n",
    "    return unigrams + bigrams\n",
    "\n",
    "template_tag_context_ngrams(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('../../data/output_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = Template(temp_df.loc[0, 'templates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.groups()[2] for i in re.finditer(asdf.pattern, asdf.template)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['player_position'] = full_df['player_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[2] for i in re.findall(asdf.pattern, asdf.template)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['player_position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders_qb = Counter()\n",
    "placeholders_rb = Counter()\n",
    "placeholders_wr = Counter()\n",
    "placeholders_te = Counter()\n",
    "\n",
    "\n",
    "for row in temp_df.itertuples():\n",
    "    placeholders_used = [i[2] for i in re.findall(Template(row.templates).pattern, row.templates)]\n",
    "    placeholders_used = list(set(placeholders_used))\n",
    "    placeholders_used.sort()\n",
    "    placeholders_used = str(placeholders_used)\n",
    "    \n",
    "    if row.player_position == 'QB':\n",
    "        placeholders_qb[placeholders_used] += 1\n",
    "    elif row.player_position == 'RB':\n",
    "        placeholders_rb[placeholders_used] += 1\n",
    "    elif row.player_position == 'WR':\n",
    "        placeholders_wr[placeholders_used] += 1\n",
    "    elif row.player_position == 'TE':\n",
    "        placeholders_te[placeholders_used] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders_rb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fantasy_nlg]",
   "language": "python",
   "name": "conda-env-fantasy_nlg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
