{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import re\n",
    "from string import Template\n",
    "import ftfy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fantasy_nlg.data_utils import create_news_stats_dataset, create_inverted_news_dict, get_teams\n",
    "from fantasy_nlg.spacy_utils import load_spacy_model\n",
    "from fantasy_nlg.generate_templates import GenerateTemplates, get_context, get_context_tags, text_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load_spacy_model('../../data/teams_aliases.txt', '../../data/player_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_stats_df = create_news_stats_dataset('../../data/player_news.csv', '../../data/football_db_player_stats.csv',\n",
    "#                                          '../../data/news_and_stats.csv')\n",
    "news_stats_df = pd.read_csv('../../data/news_and_stats.csv')\n",
    "\n",
    "template_generator = GenerateTemplates(nlp, '../../data/teams_aliases.txt', vectorizer=None, clf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create template disambiguation test data\n",
    "Perform exact matching on week 13 news. Manually replace temp_var_* tags with the correct tag.  \n",
    "\n",
    "Remove all news entries not related to game performance summary or that contain information we couldn't know.  \n",
    "* ex: injury updates  \n",
    "\n",
    "If a token should be tagged but isn't then tag it  \n",
    "* ex: NER incorrectly parses team name  \n",
    "* ex: data says rec_yards=4, but text says \"3 yard reception\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stats_df = pd.read_csv('../../data/news_and_stats.csv')\n",
    "test_data = news_stats_df[lambda df: df['week'] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = template_generator.create_training_data(test_data, '../../data/intermediate_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all temp_var_* tags replaced? Are there any mispelled tags?\n",
    "test_df = pd.read_csv('../../data/intermediate_templates_test_john.csv')\n",
    "\n",
    "for row, temp_str in enumerate(test_df['templates']):\n",
    "    template = Template(temp_str)\n",
    "    tags = [i[2] for i in re.findall(template.pattern, template.template)]\n",
    "    for tag in tags:\n",
    "        if tag not in template_generator.data_cols:\n",
    "            print('Row {}, tag {}, full text \"{}\"'.format(row, tag, template.template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../data/intermediate_templates_test_john.csv')\n",
    "test_news_stats_df = pd.merge(news_stats_df, test_df, how='right', on='report')\n",
    "test_news_stats_df.to_csv('../../data/template_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "Levenshtein distance between templates split on slot/non-slot boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_dist(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measure(predictions, actuals):\n",
    "    template_split = re.compile(r'\\$\\{([_a-z][_a-z0-9]*)\\}')\n",
    "    dist_sum = 0\n",
    "    count = 0\n",
    "    for predicted, actual in zip(predictions, actuals):\n",
    "        count += 1\n",
    "        dist_sum += levenshtein_dist(\n",
    "            re.split(template_split, predicted),\n",
    "            re.split(template_split, actual)\n",
    "        )\n",
    "\n",
    "    print('Total:', dist_sum)\n",
    "    print('Avg:', dist_sum / count)\n",
    "    return dist_sum / count\n",
    "\n",
    "# Example:    performance_measure(other_df['templates'], test_news_stats_df['templates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Exact match and choose randomly when ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_predictor(token, tag_choice):\n",
    "    return random.choice(tag_choice)\n",
    "\n",
    "baseline_generator = GenerateTemplates(nlp, '../../data/teams_aliases.txt', vectorizer=None, clf=None)\n",
    "baseline_generator.prediction_func = baseline_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_stats_df = pd.read_csv('../../data/template_test_data.csv')\n",
    "actual_templates = test_news_stats_df['templates']\n",
    "test_news_stats_df.drop(columns='templates', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_generator.template_transformer(test_news_stats_df, '../../data/baseline_templates.csv')\n",
    "performance_measure(\n",
    "    pd.read_csv('../../data/baseline_templates.csv')['templates'], \n",
    "    actual_templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stats_df = pd.read_csv('../../data/news_and_stats.csv')\n",
    "news_stats_df = news_stats_df[lambda df: df['week'] < 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_stats_df.shape)\n",
    "news_stats_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_training_set = template_generator.create_training_data(news_stats_df, '../../data/intermediate_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_ngrams = []\n",
    "context_tags = []\n",
    "true_tags = []\n",
    "\n",
    "for sample in token_training_set:\n",
    "    # context ngrams\n",
    "    bigrams = []\n",
    "    for bigram in sample[1]:\n",
    "        if len(bigram) != 0:\n",
    "            bigrams.append(' '.join(bigram))\n",
    "    context_ngrams.append(bigrams)\n",
    "    \n",
    "    # context tags\n",
    "    tags = []\n",
    "    for tag in sample[2]:\n",
    "        if tag is not None:\n",
    "            tags.append(tag)\n",
    "    context_tags.append(tags)\n",
    "    \n",
    "    # True tag\n",
    "    true_tags.append(sample[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def noop(d):\n",
    "    return d\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(lowercase=False, tokenizer=noop, analyzer=noop)\n",
    "tag_vectorizer = CountVectorizer(lowercase=False, tokenizer=noop, analyzer=noop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngrams = ngram_vectorizer.fit_transform(context_ngrams)\n",
    "X_tags = tag_vectorizer.fit_transform(context_tags)\n",
    "\n",
    "y_tags = np.array([template_generator.data_col_idx[i] for i in true_tags])\n",
    "print(X_ngrams.shape, X_tags.shape, y_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "ngram_clf = MultinomialNB(alpha=0.5)\n",
    "ngram_clf.fit(X_ngrams, y_tags)\n",
    "predictions = ngram_clf.predict(X_ngrams)\n",
    "print('Acc:', np.sum(predictions == y_tags) / len(y_tags))\n",
    "\n",
    "tag_clf = MultinomialNB(alpha=0.5)\n",
    "tag_clf.fit(X_tags, y_tags)\n",
    "predictions = tag_clf.predict(X_tags)\n",
    "print('Acc:', np.sum(predictions == y_tags) / len(y_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.Series([template_generator.idx_data_col[i] for i in y_tags], name='Actual')\n",
    "y_pred = pd.Series([template_generator.idx_data_col[i] for i in predictions], name='Predicted')\n",
    "pd.crosstab(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_generator.create_prediction_func(ngram_vectorizer, ngram_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = template_generator.template_transformer(news_stats_df, '../../data/nbmodel_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with Pickle\n",
    "TemplateModel = namedtuple('TemplateModel', ['vectorizer', 'classifier'])\n",
    "# Need to copy noop() function from vectorizer when unpickling (see vectorizer args: tokenizer, analyzer)\n",
    "with open('../../models/ngram_nb.pkl', 'wb') as f:\n",
    "    pickle.dump(TemplateModel(ngram_vectorizer, ngram_clf), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplateModel = namedtuple('TemplateModel', ['vectorizer', 'classifier'])\n",
    "def noop(d):\n",
    "    return d\n",
    "\n",
    "with open('../../models/ngram_nb.pkl', 'rb') as f:\n",
    "    TemplateModel = pickle.load(f)\n",
    "    \n",
    "template_generator.create_prediction_func(TemplateModel.vectorizer, TemplateModel.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_stats_df = pd.read_csv('../../data/template_test_data.csv')\n",
    "actual_templates = test_news_stats_df['templates']\n",
    "test_news_stats_df.drop(columns='templates', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_templates = template_generator.template_transformer(test_news_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_measure(\n",
    "    #pd.read_csv('../../data/output_templates.csv')['templates'],\n",
    "    out_templates,\n",
    "    actual_templates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('../../data/output_templates.csv')\n",
    "placeholders_qb = Counter()\n",
    "placeholders_rb = Counter()\n",
    "placeholders_wr = Counter()\n",
    "placeholders_te = Counter()\n",
    "\n",
    "\n",
    "for row in temp_df.itertuples():\n",
    "    placeholders_used = [i[2] for i in re.findall(Template(row.templates).pattern, row.templates)]\n",
    "    placeholders_used = list(set(placeholders_used))\n",
    "    placeholders_used.sort()\n",
    "    placeholders_used = str(placeholders_used)\n",
    "    \n",
    "    if row.player_position == 'QB':\n",
    "        placeholders_qb[placeholders_used] += 1\n",
    "    elif row.player_position == 'RB':\n",
    "        placeholders_rb[placeholders_used] += 1\n",
    "    elif row.player_position == 'WR':\n",
    "        placeholders_wr[placeholders_used] += 1\n",
    "    elif row.player_position == 'TE':\n",
    "        placeholders_te[placeholders_used] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random baseline (at tag level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_correct = 0\n",
    "num_tag_ambig = 0\n",
    "num_tag_unambig = 0\n",
    "num_docs = 0\n",
    "num_doc_ambig = 0\n",
    "num_doc_unambig = 0\n",
    "tag_conflicts = Counter()\n",
    "unmatched_num = 0\n",
    "\n",
    "for idx, doc, news_dict, inverted_news_dict in template_generator.doc_preprocessing(news_stats_df):\n",
    "    num_docs += 1\n",
    "    ambig_doc = False\n",
    "    unambig_doc = False\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in inverted_news_dict:\n",
    "            if type(inverted_news_dict[token.text]) is list:\n",
    "                random_correct += 1 / len(inverted_news_dict[token.text])\n",
    "                num_tag_ambig += 1\n",
    "                ambig_doc = True\n",
    "                tag_conflicts[token.text] += 1\n",
    "            else:\n",
    "                random_correct += 1\n",
    "                num_tag_unambig += 1\n",
    "                unambig_doc = True\n",
    "        elif token.pos_ == 'NUM':\n",
    "            unmatched_num += 1\n",
    "    \n",
    "    if ambig_doc:\n",
    "        num_doc_ambig += 1\n",
    "    if unambig_doc:\n",
    "        num_doc_unambig += 1\n",
    "\n",
    "print('Total tags: {}, Ambiguous tags: {}, Random accuracy: {}'.format(\n",
    "    num_tag_ambig + num_tag_unambig, num_tag_ambig, random_correct / (num_tag_ambig + num_tag_unambig)\n",
    "))\n",
    "print('Total docs: {}, Docs with ambiguous tags: {}, Percent with ambiguous tags: {}'.format(\n",
    "    num_docs, num_doc_ambig, num_doc_ambig / num_docs\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fantasy_nlg]",
   "language": "python",
   "name": "conda-env-fantasy_nlg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
