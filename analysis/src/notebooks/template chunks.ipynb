{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from itertools import chain\n",
    "import ftfy\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from string import Template\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fantasy_nlg.data_utils import create_news_stats_dataset, create_inverted_news_dict, get_teams\n",
    "from fantasy_nlg.spacy_utils import load_spacy_model\n",
    "from fantasy_nlg.generate_templates import GenerateTemplates, get_context, get_context_tags, text_normalization, record_features\n",
    "from fantasy_nlg.news_nlg import NewsGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load_spacy_model('../../data/teams_aliases.txt', '../../data/player_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_stats_df = create_news_stats_dataset('../../data/player_news.csv', '../../data/football_db_player_stats.csv',\n",
    "#                                          '../../data/news_and_stats.csv')\n",
    "news_stats_df = pd.read_csv('../../data/news_and_stats.csv')\n",
    "news_stats_df = news_stats_df[lambda df: df['week'] < 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplateModel = namedtuple('TemplateModel', ['vectorizer', 'classifier'])\n",
    "def noop(d):\n",
    "    return d\n",
    "\n",
    "with open('../../models/ngram_nb.pkl', 'rb') as f:\n",
    "    TemplateModel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_generator = GenerateTemplates(nlp, '../../data/teams_aliases.txt', vectorizer=TemplateModel.vectorizer, clf=TemplateModel.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = template_generator.template_transformer(news_stats_df, '../../data/temp_templates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, chunk_training_dict = template_generator.template_transformer(news_stats_df, '../../data/temp_templates.csv', chunking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove infrequent template chunks\n",
    "teams = get_teams('../../data/teams_aliases.txt')\n",
    "\n",
    "for k, v in chunk_training_dict.items():\n",
    "    if k in ['passing', 'rushing', 'receptions', 'game']:\n",
    "        large_counts = Counter(v['y'])\n",
    "        large_counts = [temp for temp, cnt in large_counts.items() if cnt > 2]\n",
    "        large_filter = [(y in large_counts) for y in v['y']]\n",
    "        y_temp = np.row_stack(v['y'])[large_filter]\n",
    "        X_temp = np.row_stack(v['X'])[large_filter]\n",
    "        chunk_training_dict[k] = {'X': X_temp, 'y': y_temp}\n",
    "    elif k == 'game':\n",
    "        large_counts = Counter(v['y'])\n",
    "        large_counts = [temp for temp, cnt in large_counts.items() if cnt > 2]\n",
    "        \n",
    "        large_filter = []\n",
    "        for y in v['y']:\n",
    "            team_present = False\n",
    "            for team in teams:\n",
    "                if team in y:\n",
    "                    team_present = True\n",
    "                    break\n",
    "            large_filter.append(y in large_counts and not team_present)\n",
    "        \n",
    "        y_temp = np.row_stack(v['y'])[large_filter]\n",
    "        X_temp = np.row_stack(v['X'])[large_filter]\n",
    "        chunk_training_dict[k] = {'X': X_temp, 'y': y_temp}\n",
    "    else:\n",
    "        chunk_training_dict[k] = {'X': np.row_stack(v['X']), 'y': np.row_stack(v['y'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in chunk_training_dict.items():\n",
    "    print(k, v['X'].shape, v['y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#record_type_enc = OneHotEncoder()\n",
    "#record_type_enc.fit_transform(record_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../data/template_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_gold_standard(df):\n",
    "    gold_chunks = []\n",
    "    for row in df.iterrows():\n",
    "        news_dict = row[1].to_dict()\n",
    "        inverted_news_dict = create_inverted_news_dict(news_dict, \n",
    "                                                       template_generator.data_cols, \n",
    "                                                       template_generator.team_id_dict,\n",
    "                                                       template_generator.id_team_dict)\n",
    "\n",
    "        normalized_text = text_normalization(news_dict['report'])\n",
    "\n",
    "        doc = nlp(normalized_text)\n",
    "\n",
    "        doc = template_generator.template_tagging(doc, inverted_news_dict, training=False)\n",
    "        news_template = template_generator.doc_to_template(doc)\n",
    "        gold_chunks.append(['START'] + [c.label_ for c in template_generator.chunker(doc)] + ['END'])\n",
    "    \n",
    "    return gold_chunks\n",
    "\n",
    "gold_chunks = chunk_gold_standard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_dist(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def performance_measure(predictions, actuals):\n",
    "    dist_sum = 0\n",
    "    count = 0\n",
    "    for predicted, actual in zip(predictions, actuals):\n",
    "        count += 1\n",
    "        dist_sum += levenshtein_dist(predicted, actual)\n",
    "\n",
    "    print('Total:', dist_sum)\n",
    "    print('Avg:', dist_sum / count)\n",
    "    return dist_sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_clf = LogisticRegression(C=10.0, max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_clf.fit(chunk_training_dict['record']['X'], chunk_training_dict['record']['y'].ravel())\n",
    "preds = record_clf.predict(chunk_training_dict['record']['X'])\n",
    "\n",
    "print(accuracy_score(chunk_training_dict['record']['y'].ravel(), preds))\n",
    "print(f1_score(chunk_training_dict['record']['y'].ravel(), preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict recard sequences\n",
    "pred_records = []\n",
    "\n",
    "for row in test_data[template_generator.data_cols].iterrows():\n",
    "    news_dict = row[1].to_dict()\n",
    "    record_list = ['START']\n",
    "    count = 0\n",
    "    while record_list[-1] != 'END' and count < 10:\n",
    "        count += 1\n",
    "        features = record_features(record_list, news_dict, template_generator.record_types, template_generator.data_cols)\n",
    "        record_list.append(record_clf.predict(features.reshape(1, -1))[0])\n",
    "    pred_records.append(record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_measure(pred_records, gold_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best performing model\n",
    "with open('../../models/record_selection_lr.pkl', 'wb') as f:\n",
    "    pickle.dump(record_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template data\n",
    "template_X = np.concatenate((chunk_training_dict['game']['X'], chunk_training_dict['passing']['X'], chunk_training_dict['receptions']['X'], \n",
    "                             chunk_training_dict['rushing']['X']))\n",
    "template_y = np.concatenate((chunk_training_dict['game']['y'], chunk_training_dict['passing']['y'], chunk_training_dict['receptions']['y'], \n",
    "                             chunk_training_dict['rushing']['y']))\n",
    "\n",
    "shuffle = np.random.permutation(template_y.shape[0])\n",
    "template_X = template_X[shuffle]\n",
    "template_y = template_y[shuffle]\n",
    "print(template_X.shape, template_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "parameters = {'n_neighbors':[1, 3, 5, 10, 15], 'weights':('uniform', 'distance')}\n",
    "knn_clf = KNeighborsClassifier()\n",
    "clf = GridSearchCV(knn_clf, parameters, cv=5)\n",
    "clf.fit(template_X, template_y.ravel())\n",
    "clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "knn_clf.fit(template_X, template_y.ravel())\n",
    "preds = knn_clf.predict(template_X)\n",
    "\n",
    "print(accuracy_score(template_y.ravel(), preds))\n",
    "print(f1_score(template_y.ravel(), preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "parameters = {'n_estimators':[100, 300, 1000, 3000], 'min_samples_leaf':[1, 3, 5]}\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(rf_clf, parameters, cv=5)\n",
    "clf.fit(template_X, template_y.ravel())\n",
    "clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=3000, min_samples_leaf=3, random_state=42)\n",
    "rf_clf.fit(template_X, template_y.ravel())\n",
    "preds = rf_clf.predict(template_X)\n",
    "\n",
    "print(accuracy_score(template_y.ravel(), preds))\n",
    "print(f1_score(template_y.ravel(), preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_col_types(text):\n",
    "    template_regex = re.compile(r'\\$\\{([_a-z][_a-z0-9]*)\\}')\n",
    "    tags = re.findall(template_regex, text)\n",
    "    try:\n",
    "        return template_generator.data_col_to_type[tags[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def record_type_mask(classes):\n",
    "    record_types = []\n",
    "    for cls in classes:\n",
    "        record_types.append(get_data_col_types(cls))\n",
    "    return record_types\n",
    "\n",
    "record_types = record_type_mask(rf_clf.classes_)\n",
    "record_types = np.array(record_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf_clf.predict_proba(template_X)\n",
    "\n",
    "tmplt_out = []\n",
    "for pred, rt in zip(preds, record_type):\n",
    "    record_mask = np.where(record_types == rt, 1.0, 0.0)\n",
    "    tmplt_out.append(rf_clf.classes_[np.argmax(pred * record_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_clf = LogisticRegression(C=10.0, max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf_clf = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "asdf_clf.fit(template_X, template_y.ravel())\n",
    "preds = asdf_clf.predict(template_X)\n",
    "\n",
    "print(accuracy_score(template_y.ravel(), preds))\n",
    "print(f1_score(template_y.ravel(), preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best performing model\n",
    "with open('../../models/template_selection_knn.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../data/template_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle models\n",
    "with open('../../models/record_selection_lr.pkl', 'rb') as f:\n",
    "    record_clf = pickle.load(f)\n",
    "\n",
    "with open('../../models/template_selection_knn.pkl', 'rb') as f:\n",
    "    template_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_generator = NewsGenerator('../../data/teams_aliases.txt', record_clf, template_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_output, template_output, news_output = news_generator.doc_processing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'record': record_output, 'template': template_output, 'news_update': news_output})\n",
    "output_df.to_csv('../../data/test_news_updates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = news_stats_df.loc[550].to_dict()\n",
    "inverted_news_dict = create_inverted_news_dict(news_dict, \n",
    "                                               template_generator.data_cols, \n",
    "                                               template_generator.team_id_dict,\n",
    "                                               template_generator.id_team_dict)\n",
    "\n",
    "normalized_text = text_normalization(news_dict['report'])\n",
    "\n",
    "doc = nlp(normalized_text)\n",
    "\n",
    "doc = template_generator.template_tagging(doc, inverted_news_dict, training=False)\n",
    "news_template = template_generator.doc_to_template(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_generator.chunker(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(template_generator.data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_templates = pd.read_csv('../../data/nbmodel_templates.csv')\n",
    "output_templates.rename(index=str, columns={'reports': 'report'}, inplace=True)\n",
    "asdf = pd.merge(news_stats_df, output_templates, how='right', on='report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique chunks\n",
    "get_data_col_types('${player_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_output = pd.read_csv('../../data/nbmodel_templates.csv')\n",
    "\n",
    "chunk_dict = dict()\n",
    "for chunks in nb_output['template_chunks']:\n",
    "    for chunk in eval(chunks):\n",
    "        chunk_type = get_data_col_types(chunk)\n",
    "        if chunk_type not in chunk_dict:\n",
    "            chunk_dict[chunk_type] = []\n",
    "        #print(chunk_dict[chunk_type], '\\t', chunk_type, '\\t', chunk_dict[chunk_type].append(chunk))\n",
    "        chunk_dict[chunk_type].append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_counters = dict()\n",
    "for chunk_type in chunk_dict.keys():\n",
    "    chunk_counters[chunk_type] = Counter(chunk_dict[chunk_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(chunk_counters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunk_counters['game'].keys()))\n",
    "count = 0\n",
    "num_samples = 0\n",
    "for k, v in chunk_counters['game'].items():\n",
    "    if v > 2:\n",
    "        count += 1\n",
    "        num_samples += v\n",
    "        print(k, ':', v)\n",
    "print(count, num_samples)\n",
    "#chunk_counters['game']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fantasy_nlg]",
   "language": "python",
   "name": "conda-env-fantasy_nlg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
